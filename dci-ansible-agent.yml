---
# Initial Step:
#
# Schedule a new job giving a specific topic and specifying the remote CI.
# The return of this action contains all the data associated with the job,
# we hence register those data for later consumptions
#
- hosts: localhost
  tasks:
    - name: Schedule a new job
      dci_job:
        components: '{{ dci_components|default([]) }}'
        topic: '{{ dci_topic }}'
        # id: '88fbd033-07b5-4cce-bb71-f320276cb0d0'
        # embed: 'remoteci,rconfiguration,topic,components'
      register: job_informations

    - set_fact:
        job_id: '{{ hostvars.localhost.job_informations.job.id }}'
        components: '{{ hostvars.localhost.job_informations.job.components }}'
    - name: Set the metadata
      dci_job:
        id: "{{ job_informations.job.id }}"
        metadata: '{{ dci_metadata }}'
      when: dci_metadata is defined and dci_metadata

- name: 'Sanity check'
  hosts: localhost
  vars:
    dci_status: 'new'
  tasks:
    - block:
      - name: Check the jumpbox sanity
        include: plays/sanity_check.yml

      - include: plays/purge_cache.yml

      - include: plays/prepare_local_mirror.yml

      - include: plays/fetch_bits.yml

      - set_fact:
          dci_status: 'pre-run'

      - include: '{{ dci_config_dir }}/hooks/pre-run.yml'

      rescue:
      - include: "{{ dci_config_dir }}/hooks/teardown.yml"
        when: dci_teardown_on_failure is defined and dci_teardown_on_failure

      - include: plays/failure.yml


# Run state
#
# User is free to do whaterver she needs before entering running state.
# Usually this is used to provision both undercloud and the overcloud.
#
- name: 'Provision the undercloud and the overcloud'
  hosts: localhost
  vars:
    dci_status: 'running'
    next_topic:
      OSP8: 'OSP9'
      OSP9: 'OSP10'
      OSP10: 'OSP11'
      OSP11: 'OSP12'
      OSP12: 'OSP13'
      OSP13: 'OSP14'
  tasks:
    - block:
      - set_fact:
          # job_informations may be redefined below. We need a set_fact here to
          # store a static result.
          update_playbook: '{{ dci_config_dir }}/hooks/update_{{ job_informations.job.topic.name }}.yml'
          upgrade_playbook: '{{ dci_config_dir }}/hooks/upgrade_from_{{ job_informations.job.topic.name }}_to_{{ next_topic[job_informations.job.topic.name] }}.yml'
          # NOTE(Gonéri): Workaround for https://github.com/ansible/ansible/issues/23733
          # tripleo-inventory depends on the common role, however the role is not include
          # has it should when we use the include_role directive.
          # Here we manually declare the deploy_supplemental_node variable to avoid an
          # undefined value during the role execution
          deploy_supplemental_node: No
      - stat:
          path: '{{ update_playbook }}'
        register: update_playbook_stat
      - stat:
          path: '{{ upgrade_playbook }}'
        register: upgrade_playbook_stat
      - include: '{{ dci_config_dir }}/hooks/running.yml'

      - include: plays/register_undercloud_host.yml

      - include: plays/run_tests.yml
        delegate_to: undercloud

      - name: Update the deployment
        block:
        - include: plays/collect_logs.yml
        - include: plays/upload_logs.yml

        - dci_job:
            id: '{{ job_informations.job.id }}'
            update: True
          register: new_job_informations
        # NOTE(Gonéri): avoid https://github.com/ansible/ansible/issues/4297
        - set_fact:
            job_informations: '{{ new_job_informations }}'
          when: new_job_informations.skipped is not defined



        - include: '{{ update_playbook }}'
        - include: plays/run_tests.yml
          delegate_to: undercloud
        when: update_playbook_stat.stat.exists

      - name: Upgrade the deployment to the next release
        block:
        - include: plays/collect_logs.yml
        - include: plays/upload_logs.yml

        - dci_job:
            id: '{{ job_informations.job.id }}'
            status: success

        - dci_job:
            id: '{{ job_informations.job.id }}'
            upgrade: True
          register: new_job_informations
        - set_fact:
            job_informations: '{{ new_job_informations }}'
          when: new_job_informations.skipped is not defined

        - set_fact:
            dci_status: 'running'

        - include: '{{ upgrade_playbook }}'
        - include: plays/run_tests.yml
          delegate_to: undercloud

        when: upgrade_playbook_stat.stat.exists

      rescue:
      # Included a second time in case of failure in hooks/running.yml,
      # this is required for collect-logs to work
      - include: plays/register_undercloud_host.yml
      - include: plays/failure.yml

      always:
      - include: plays/collect_logs.yml
        ignore_errors: yes
      - include: plays/upload_logs.yml
        ignore_errors: yes

      - include: "{{ dci_config_dir }}/hooks/teardown.yml"
        when: dci_teardown_on_failure is defined and dci_teardown_on_failure


# Success state
#
# User is free to do whaterver she needs before entering pre-run state.
# Usually this is used to teardown the plateform
#
- name: 'Successful installation'
  hosts: localhost
  vars:
    dci_status: 'success'
  tasks:
    - include: "{{ dci_config_dir }}/hooks/success.yml"

    - include: "{{ dci_config_dir }}/hooks/teardown.yml"
