---
# Initial Step:
#
# Schedule a new job giving a specific topic and specifying the remote CI.
# The return of this action contains all the data associated with the job,
# we hence register those data for later consumptions
#
- hosts: localhost
  vars:
    dci_login: '{{ dci_login }}'
    dci_password: '{{ dci_password }}'
  tasks:
    - set_fact:
        metadata:
          experimental: '{{ dci_experimental is defined and dci_experimental }}'

    - block:
      - include: "{{ dci_config_dir }}/hooks/teardown.yml"

      - name: Schedule a new job
        dci_job:
          topic: '{{ dci_topic }}'
          remoteci: '{{ dci_remoteci }}'
        register: job_schedule
      when: not dci_upgrade

    - block:
      - name: Fail if previous_job_id is not set
        fail:
          msg: 'previous_job_id is not defined'
        when: previous_job_id is not defined

      - name: Schedule an upgrade job
        dci_job:
          id: '{{ previous_job_id }}'
          upgrade: true
        register: job_upgrade
      when: dci_upgrade

    - set_fact:
        job_informations: '{{ job_upgrade if dci_upgrade else job_schedule }}'

    - name: Set the metadata
      dci_job:
        id: "{{ job_informations['job_id'] }}"
        metadata: '{{ metadata }}'


# New state
#
# User is free to do whaterver she needs before entering pre-run state.
# Usually this is used to sync the components the job is relying on.
#
- hosts: localhost
  vars:
    dci_status: 'new'
    dci_comment: 'Creating the local mirrors and synchronizing them'
    components: "{{ job_informations['components'] }}"
    job_id: "{{ job_informations['job_id'] }}"
  tasks:
    - block:
      - name: Ensure Apache is installed
        package:
          name: 'httpd'
          state: 'present'
        become: true
      - name: Ensure Apache is running
        systemd:
          name: 'httpd'
          state: 'started'
        become: true
      - name: Ensure python-firewall is installed
        package:
          name: 'python-firewall'
          state: 'present'
        become: true
      - name: Open port 80 on the firewall
        firewalld:
          zone: public
          service: http
          permanent: true
          state: enabled
        register: result
        failed_when: false
        become: true
      - name: Drop existing repo dir
        file:
          path: '{{ dci_mirror_location }}/dci_repo'
          state: absent
        become: true
      - name: Ensure proper directories are created
        file:
          path: '{{ item }}'
          state: directory
          owner: dci-ansible-agent
          group: dci-ansible-agent
        with_items:
          - '{{ dci_cache_dir }}'
          - '{{ dci_mirror_location }}/dci_repo'
        become: true
      - name: Identify outdated files from the cache
        find:
          path: '{{ dci_cache_dir }}'
          age: '4w'
        register: old_files_found
      - name: Purge old files from the local cache
        file:
          path: '{{ item.path }}'
          state: absent
        with_items: '{{ old_files_found.files }}'
      - name: Retrieve component
        dci_component:
          dest: '{{ dci_cache_dir }}/{{ item["id"] }}.tar'
          id: '{{ item["id"] }}'
        with_items: "{{ components }}"

      - name: Unarchive component
        unarchive:
          src: '{{ dci_cache_dir }}/{{ item["id"] }}.tar'
          dest: '{{ dci_mirror_location }}/dci_repo'
          remote_src: True
        with_items: "{{ components }}"

      - name: Prepare the YUM repo file
        yum_repository:
          name: '{{ item["canonical_project_name"] }}'
          description: '{{ item["canonical_project_name"] }}'
          baseurl: '{{ dci_baseurl }}/dci_repo/{{ item["canonical_project_name"] }}/'
          gpgcheck: no
          file: '{{ dci_mirror_location }}/dci_repo/dci_repo'
        with_items: '{{ job_informations["components"] }}'

      - name: Generating RSA key for stack
        user:
          name: 'dci-ansible-agent'
          generate_ssh_key: 'yes'

      rescue:
      - include: failure.yml


# Pre-run state
#
# User is free to do whaterver she needs before entering pre-run state.
# Usually this is used to boot (not provision) the undercloud
#
- hosts: localhost
  vars:
    dci_status: 'pre-run'
    dci_comment: 'Spawning the undercloud'
    job_id: "{{ job_informations['job_id'] }}"
  tasks:
    - block:
      - include: '{{ dci_config_dir }}/hooks/pre-run.yml'
      - name: Add undercloud
        add_host:
          name: "undercloud"
          groups: "undercloud"
          ansible_fqdn: "{{ undercloud_ip }}"
          ansible_user: "stack"
          ansible_host: "{{ undercloud_ip }}"
          ansible_password: "{{ undercloud_password }}"

      rescue:
      - include: failure.yml

# Run state
#
# User is free to do whaterver she needs before entering running state.
# Usually this is used to provision both undercloud and the overcloud.
#
- hosts: localhost
  vars:
    dci_status: 'running'
    dci_comment: 'Provision the undercloud and the overcloud'
    job_id: "{{ job_informations['job_id'] }}"
  tasks:
    - block:
      - include: '{{ dci_config_dir }}/hooks/running.yml'

      rescue:
      - include: failure.yml


- hosts: localhost
  vars:
    dci_status: 'post-run'
    dci_comment: 'add the undercloud node to host list'
    job_id: "{{ job_informations['job_id'] }}"
  tasks:
    - block:
        - name: Set authorized key of stack@undercloud (1/2)
          authorized_key:
            user: stack
            state: present
            key: "{{ lookup('file', '/var/lib/dci-ansible-agent/.ssh/id_rsa.pub') }}"
          delegate_to: undercloud
        - slurp:
            src: /home/stack/.ssh/id_rsa.pub
          register: id_rsa_pub
          delegate_to: undercloud
        - name: Set authorized key of stack@undercloud (2/2)
          authorized_key:
            user: stack
            state: present
            key: "{{ id_rsa_pub.content | b64decode }}"
          delegate_to: undercloud
        - name: check the privilege of the ~stack/.ssh/config file
          file:
            path: /home/stack/.ssh/config
            owner: stack
            group: stack
            mode: 0600
          delegate_to: undercloud
        - name: Add the jumpbox in the virthost group
          add_host:
            name: "localhost"
            groups: "virthost"

        - name: Ensure ~/.quickstart is a thing to please tripleo-inventory
          file:
            name: ~/.quickstart
            state: directory

        - name: Reuse the user ssh key
          copy:
            remote_src: True
            src: '/var/lib/dci-ansible-agent/.ssh/id_rsa'
            dest: '/var/lib/dci-ansible-agent/.quickstart/id_rsa_undercloud'
            mode: '0600'

        - name: Add undercloud in the /etc/hosts
          lineinfile:
            dest: /etc/hosts
            regexp: '^\d+.*\sundercloud'
            line: '{{ undercloud_ip }} undercloud'
            owner: root
            group: root
            mode: 0644
          become: True
      rescue:
      - include: failure.yml


# Post-run state
#
# User is free to do whaterver she needs before entering post-run state.
# Usually this is used to run tests, certifications, etc...
#
- name: 'Running tests'
  hosts: 'undercloud'
  vars:
    dci_status: 'post-run'
    rhos_release:
      OSP8: 'liberty'
      OSP9: 'mitaka'
      OSP10: 'newton'
      OSP11: 'ocata'
  pre_tasks:
    - name: Set the OpenStack release name
      set_fact:
        release: "{{ rhos_release[dci_topic] }}"

    - name: Copy tempest config overrides
      copy:
        src: tempest-overrides.conf
        dest: ~/tempest-deployer-input.conf

    - name: Install tempest dependencies # review 442558
      become: true
      yum:
        name: python-junitxml

    - name: Get stack dump script
      copy:
        src: tripleo-stack-dump
        dest: ~/tripleo-stack-dump
        mode: 0755

    - name: Execure tripleo-stack-dump
      shell: source ~/stackrc && ./tripleo-stack-dump overcloud

    - name: Fetch dump result
      fetch:
        src: tripleo-stack-dump.json
        dest: ~/tripleo-stack-dump.json
        flat: yes
  roles:
    - certification
    - tripleo-inventory
    - { role: validate-tempest, tempest_exit_on_failure: False }

- name: Collect logs
  hosts: 'undercloud'
  vars:
    dci_status: 'post-run'
    artcl_tar_gz: true
    artcl_gzip_only: false
  roles:
    - collect-logs

- name: 'Upload files'
  hosts: localhost
  vars:
    dci_status: 'post-run'
  tasks:
    - name: Upload stack details
      dci_job:
        id: "{{ job_informations['job_id'] }}"
        configuration: "{{ lookup('file', '~/tripleo-stack-dump.json') }}"

    # NOTE(dsavinea) : We need to ignore the errors because cert.txt
    # is only present for OSP10 for the moment.
    - name: Upload results
      dci_file:
        path: '{{ item.path }}'
        name: '{{ item.name }}'
        job_id: "{{ job_informations['job_id'] }}"
        mime: '{{ item.mime }}'
      with_items:
        - {'name': 'Tempest', 'path': '/var/lib/dci-ansible-agent/.quickstart/nosetests.xml', 'mime': 'application/junit'}
        - {'name': 'Certification', 'path': '/var/lib/dci-ansible-agent/.quickstart/cert.txt', 'mime': 'text/plain'}
      ignore_errors: yes

    - name: Upload logs
      dci_file:
        path: "/var/lib/dci-ansible-agent/.quickstart/collected_files/{{ item }}.tar.gz"
        name: "{{ item }}.tar.gz"
        job_id: "{{ job_informations['job_id'] }}"
        mime: "application/x-compressed"
      with_items:
        - "undercloud"
#        - "{{ groups['overcloud'] }}"

    - name: Clean result files
      file:
        name: '{{ item }}'
        state: absent
      with_items:
        - "~/nosetests.xml"
        - "~/results.subunit"
        - "~/tempest.html"
        - "~/tripleo-stack-dump.json"
        - "~/cert.txt"


# Success state
#
# User is free to do whaterver she needs before entering pre-run state.
# Usually this is used to teardown the plateform
#
- hosts: localhost
  vars:
    dci_status: 'success'
  tasks:
    - include: "{{ dci_config_dir }}/hooks/teardown.yml"
